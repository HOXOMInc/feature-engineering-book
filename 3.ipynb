{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例3-1. nグラムの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29222 368943 881620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '00', '000', '007', '00a', '00am', '00pm', '01', '02', '03']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 最初の 10,000 件のレビューを読み込む\n",
    "with open('data/yelp/yelp_academic_dataset_review.json') as f:\n",
    "    js = []\n",
    "    for i in range(10000):\n",
    "        js.append(json.loads(f.readline()))\n",
    "review_df = pd.DataFrame(js)\n",
    "\n",
    "# scikit-learn の CountVectorizer を使ってユニグラム（BoW）、\n",
    "# バイグラム、トライグラムの特徴量変換器を作成する。\n",
    "# CountVectorizer はデフォルトでは1文字の単語を無視するが、\n",
    "# これは意味のない単語を除外するため実用的である。\n",
    "# ただしここでは全ての単語を含むように設定している。\n",
    "bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# 変換器を適用し、語彙数を確認する\n",
    "bow_converter.fit(review_df['text'])\n",
    "words = bow_converter.get_feature_names()\n",
    "\n",
    "bigram_converter.fit(review_df['text'])\n",
    "bigrams = bigram_converter.get_feature_names()\n",
    "\n",
    "trigram_converter.fit(review_df['text'])\n",
    "trigrams = trigram_converter.get_feature_names()\n",
    "\n",
    "print (len(words), len(bigrams), len(trigrams))\n",
    "\n",
    "# n-グラムを確認する\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zuzu was',\n",
       " 'zuzus room',\n",
       " 'zweigel wine',\n",
       " 'zwiebel kräuter',\n",
       " 'zy world',\n",
       " 'zzed in',\n",
       " 'éclairs napoleons',\n",
       " 'école lenôtre',\n",
       " 'ém all',\n",
       " 'òc châm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 0 eye',\n",
       " '0 20 less',\n",
       " '0 39 oz',\n",
       " '0 39 pizza',\n",
       " '0 5 i',\n",
       " '0 50 to',\n",
       " '0 6 can',\n",
       " '0 75 oysters',\n",
       " '0 75 that',\n",
       " '0 75 to']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例3-2. 品詞タグ付けとチャンク化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General', 'PROPN', 'NNP']\n",
      "['Manager', 'PROPN', 'NNP']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['Petello', 'PROPN', 'NNP']\n",
      "['is', 'VERB', 'VBZ']\n",
      "['a', 'DET', 'DT']\n",
      "['good', 'ADJ', 'JJ']\n",
      "['egg', 'NOUN', 'NN']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['Not', 'ADV', 'RB']\n",
      "['to', 'PART', 'TO']\n",
      "['go', 'VERB', 'VB']\n",
      "['into', 'ADP', 'IN']\n",
      "['detail', 'NOUN', 'NN']\n",
      "[',', 'PUNCT', ',']\n",
      "['but', 'CCONJ', 'CC']\n",
      "['let', 'VERB', 'VB']\n",
      "['me', 'PRON', 'PRP']\n",
      "['assure', 'VERB', 'VB']\n",
      "['you', 'PRON', 'PRP']\n",
      "['if', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['have', 'VERB', 'VBP']\n",
      "['any', 'DET', 'DT']\n",
      "['issues', 'NOUN', 'NNS']\n",
      "['(', 'PUNCT', '-LRB-']\n",
      "['albeit', 'ADP', 'IN']\n",
      "['rare', 'ADJ', 'JJ']\n",
      "[')', 'PUNCT', '-RRB-']\n",
      "['speak', 'VERB', 'VBP']\n",
      "['with', 'ADP', 'IN']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['treat', 'VERB', 'VB']\n",
      "['the', 'DET', 'DT']\n",
      "['guy', 'NOUN', 'NN']\n",
      "['with', 'ADP', 'IN']\n",
      "['some', 'DET', 'DT']\n",
      "['respect', 'NOUN', 'NN']\n",
      "['as', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['state', 'VERB', 'VBP']\n",
      "['your', 'ADJ', 'PRP$']\n",
      "['case', 'NOUN', 'NN']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['I', 'PRON', 'PRP']\n",
      "[\"'d\", 'VERB', 'MD']\n",
      "['be', 'VERB', 'VB']\n",
      "['surprised', 'ADJ', 'JJ']\n",
      "['if', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['do', 'VERB', 'VBP']\n",
      "[\"n't\", 'ADV', 'RB']\n",
      "['walk', 'VERB', 'VB']\n",
      "['out', 'ADV', 'RB']\n",
      "['totally', 'ADV', 'RB']\n",
      "['satisfied', 'ADJ', 'JJ']\n",
      "['as', 'ADP', 'IN']\n",
      "['I', 'PRON', 'PRP']\n",
      "['just', 'ADV', 'RB']\n",
      "['did', 'VERB', 'VBD']\n",
      "['.', 'PUNCT', '.']\n",
      "['Like', 'INTJ', 'UH']\n",
      "['I', 'PRON', 'PRP']\n",
      "['always', 'ADV', 'RB']\n",
      "['say', 'VERB', 'VBP']\n",
      "['.....', 'PUNCT', 'NFP']\n",
      "['\"', 'PUNCT', '``']\n",
      "['Mistakes', 'NOUN', 'NNS']\n",
      "['are', 'VERB', 'VBP']\n",
      "['inevitable', 'ADJ', 'JJ']\n",
      "[',', 'PUNCT', ',']\n",
      "['it', 'PRON', 'PRP']\n",
      "[\"'s\", 'VERB', 'VBZ']\n",
      "['how', 'ADV', 'WRB']\n",
      "['we', 'PRON', 'PRP']\n",
      "['recover', 'VERB', 'VBP']\n",
      "['from', 'ADP', 'IN']\n",
      "['them', 'PRON', 'PRP']\n",
      "['that', 'ADJ', 'WDT']\n",
      "['is', 'VERB', 'VBZ']\n",
      "['important', 'ADJ', 'JJ']\n",
      "['\"', 'PUNCT', \"''\"]\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['\\n\\n', 'SPACE', '_SP']\n",
      "['Thanks', 'NOUN', 'NNS']\n",
      "['to', 'ADP', 'IN']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['his', 'ADJ', 'PRP$']\n",
      "['awesome', 'ADJ', 'JJ']\n",
      "['staff', 'NOUN', 'NN']\n",
      "['.', 'PUNCT', '.']\n",
      "['You', 'PRON', 'PRP']\n",
      "[\"'ve\", 'VERB', 'VB']\n",
      "['got', 'VERB', 'VBN']\n",
      "['a', 'DET', 'DT']\n",
      "['customer', 'NOUN', 'NN']\n",
      "['for', 'ADP', 'IN']\n",
      "['life', 'NOUN', 'NN']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['..........', 'PUNCT', 'NFP']\n",
      "[':', 'PUNCT', ':']\n",
      "['^', 'PUNCT', 'NFP']\n",
      "[')', 'PUNCT', '-RRB-']\n",
      "[General Manager Scott Petello, a good egg, detail, me, you, you, any issues, Scott, the guy, some respect, you, your case, I, you, I, I, Mistakes, it, we, them, Thanks, Scott, his awesome staff, You, a customer, life]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 最初の10レビューを読み込む\n",
    "with open('data/yelp/yelp_academic_dataset_review.json') as f:\n",
    "    js = []\n",
    "    for i in range(10):\n",
    "        js.append(json.loads(f.readline()))\n",
    "review_df = pd.DataFrame(js)\n",
    "\n",
    "# まずは Spacy を使った方法\n",
    "import spacy\n",
    "# 言語モデル（英語）を読み込む\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# spaCy の言語モデルを使ってテキストから Pandas Series を作成する\n",
    "doc_df = review_df['text'].apply(nlp)\n",
    "\n",
    "# spaCy は細かい品詞タグを .pos_ で、粗い品詞タグを .tag_ で提供します\n",
    "for doc in doc_df[4]:\n",
    "    print([doc.text, doc.pos_, doc.tag_])\n",
    "\n",
    "# spaCy は基本的な名詞句も .noun_chunks で提供します\n",
    "print([chunk for chunk in doc_df[4].noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('General', 'NNP'),\n",
       " ('Manager', 'NNP'),\n",
       " ('Scott', 'NNP'),\n",
       " ('Petello', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('good', 'JJ'),\n",
       " ('egg', 'NN'),\n",
       " ('Not', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('go', 'VB'),\n",
       " ('into', 'IN'),\n",
       " ('detail', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('let', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('assure', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('any', 'DT'),\n",
       " ('issues', 'NNS'),\n",
       " ('albeit', 'IN'),\n",
       " ('rare', 'NN'),\n",
       " ('speak', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('Scott', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('treat', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('guy', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('some', 'DT'),\n",
       " ('respect', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('state', 'NN'),\n",
       " ('your', 'PRP$'),\n",
       " ('case', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " (\"'d\", 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('surprised', 'VBN'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('walk', 'VB'),\n",
       " ('out', 'RP'),\n",
       " ('totally', 'RB'),\n",
       " ('satisfied', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('did', 'VBD'),\n",
       " ('Like', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('always', 'RB'),\n",
       " ('say', 'VBP'),\n",
       " ('..', 'VBP'),\n",
       " ('Mistakes', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('inevitable', 'JJ'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('how', 'WRB'),\n",
       " ('we', 'PRP'),\n",
       " ('recover', 'VBP'),\n",
       " ('from', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('important', 'JJ'),\n",
       " ('Thanks', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('Scott', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('his', 'PRP$'),\n",
       " ('awesome', 'JJ'),\n",
       " ('staff', 'NN'),\n",
       " ('You', 'PRP'),\n",
       " (\"'ve\", 'VBP'),\n",
       " ('got', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('customer', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('life', 'NN'),\n",
       " ('^', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextBlob ライブラリを使って同じことができる\n",
    "from textblob import TextBlob\n",
    "\n",
    "# TextBlob はデフォルトでは PatternTagger を使ってタグ付けを行う。\n",
    "# これは今回の例ではうまくいくが、文法の正しくない文章を含む場合は \n",
    "# NLTKTagger を使うことをおすすめする。\n",
    "blob_df = review_df['text'].apply(TextBlob)\n",
    "\n",
    "blob_df[4].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['general manager', 'scott petello', 'good egg', 'scott', \"n't walk\", '... ..', 'mistakes', 'thanks', 'scott', 'awesome staff', '... ... ...']\n"
     ]
    }
   ],
   "source": [
    "print([np for np in blob_df[4].noun_phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
